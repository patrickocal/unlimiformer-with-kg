\begin{thebibliography}{44}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Ainslie et~al.(2020)Ainslie, Ontanon, Alberti, Cvicek, Fisher, Pham,
  Ravula, Sanghai, Wang, and Yang}]{ETC}
Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher,
  Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li~Yang. 2020.
\newblock \href {https://doi.org/10.48550/ARXIV.2004.08483} {Etc: Encoding long
  and structured inputs in transformers}.

\bibitem[{Alon et~al.(2022)Alon, Xu, He, Sengupta, Roth, and
  Neubig}]{retomaton}
Uri Alon, Frank~F. Xu, Junxian He, Sudipta Sengupta, Dan Roth, and Graham
  Neubig. 2022.
\newblock \href {https://doi.org/10.48550/ARXIV.2201.12431} {Neuro-symbolic
  language modeling with automaton-augmented retrieval}.

\bibitem[{Bajaj et~al.(2021)Bajaj, Dangati, Krishna, Ashok~Kumar, Uppaal,
  Windsor, Brenner, Dotterrer, Das, and McCallum}]{bajaj-etal-2021-long}
Ahsaas Bajaj, Pavitra Dangati, Kalpesh Krishna, Pradhiksha Ashok~Kumar, Rheeya
  Uppaal, Bradford Windsor, Eliot Brenner, Dominic Dotterrer, Rajarshi Das, and
  Andrew McCallum. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.acl-srw.7} {Long document
  summarization in a low resource setting using pretrained language models}.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing: Student Research Workshop}, pages 71--80,
  Online. Association for Computational Linguistics.

\bibitem[{Beltagy et~al.(2020)Beltagy, Peters, and
  Cohan}]{beltagy2020longformer}
Iz~Beltagy, Matthew~E Peters, and Arman Cohan. 2020.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}.

\bibitem[{Borgeaud et~al.(2022)Borgeaud, Mensch, Hoffmann, Cai, Rutherford,
  Millican, Van Den~Driessche, Lespiau, Damoc, Clark
  et~al.}]{borgeaud2022improving}
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza
  Rutherford, Katie Millican, George~Bm Van Den~Driessche, Jean-Baptiste
  Lespiau, Bogdan Damoc, Aidan Clark, et~al. 2022.
\newblock Improving language models by retrieving from trillions of tokens.
\newblock In \emph{International conference on machine learning}, pages
  2206--2240. PMLR.

\bibitem[{Chen et~al.(2022)Chen, Chu, Wiseman, and
  Gimpel}]{chen-etal-2022-summscreen}
Mingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-long.589}
  {{S}umm{S}creen: A dataset for abstractive screenplay summarization}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 8602--8615,
  Dublin, Ireland. Association for Computational Linguistics.

\bibitem[{Child et~al.(2019)Child, Gray, Radford, and
  Sutskever}]{sparse-transformers}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019.
\newblock \href {https://doi.org/10.48550/ARXIV.1904.10509} {Generating long
  sequences with sparse transformers}.

\bibitem[{Choromanski et~al.(2020)Choromanski, Likhosherstov, Dohan, Song,
  Gane, Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and
  Weller}]{performers}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
  Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. 2020.
\newblock \href {https://doi.org/10.48550/ARXIV.2009.14794} {Rethinking
  attention with performers}.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova}]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
\newblock \href {https://doi.org/10.18653/v1/N19-1423} {{BERT}: Pre-training of
  deep bidirectional transformers for language understanding}.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota. Association for Computational Linguistics.

\bibitem[{Drozdov et~al.(2022)Drozdov, Wang, Rahimi, McCallum, Zamani, and
  Iyyer}]{drozdov-etal-2022-cant}
Andrew Drozdov, Shufan Wang, Razieh Rahimi, Andrew McCallum, Hamed Zamani, and
  Mohit Iyyer. 2022.
\newblock \href {https://aclanthology.org/2022.findings-emnlp.218} {You can{'}t
  pick your neighbors, or can you? when and how to rely on retrieval in the
  k{NN}-{LM}}.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2022}, pages 2997--3007, Abu Dhabi, United Arab Emirates. Association
  for Computational Linguistics.

\bibitem[{Grail et~al.(2021)Grail, Perez, and
  Gaussier}]{grail-etal-2021-globalizing}
Quentin Grail, Julien Perez, and Eric Gaussier. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.eacl-main.154} {Globalizing
  {BERT}-based transformer architectures for long document summarization}.
\newblock In \emph{Proceedings of the 16th Conference of the European Chapter
  of the Association for Computational Linguistics: Main Volume}, pages
  1792--1810, Online. Association for Computational Linguistics.

\bibitem[{Huang et~al.(2021)Huang, Cao, Parulian, Ji, and
  Wang}]{huang-etal-2021-efficient}
Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu~Wang. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.naacl-main.112} {Efficient
  attentions for long document summarization}.
\newblock In \emph{Proceedings of the 2021 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 1419--1436, Online. Association for Computational
  Linguistics.

\bibitem[{Ivgi et~al.(2022)Ivgi, Shaham, and Berant}]{sled}
Maor Ivgi, Uri Shaham, and Jonathan Berant. 2022.
\newblock \href {https://doi.org/10.48550/ARXIV.2208.00748} {Efficient
  long-text understanding with short-text models}.

\bibitem[{Izacard and Grave(2021)}]{izacard-grave-2021-leveraging}
Gautier Izacard and Edouard Grave. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.eacl-main.74} {Leveraging
  passage retrieval with generative models for open domain question answering}.
\newblock In \emph{Proceedings of the 16th Conference of the European Chapter
  of the Association for Computational Linguistics: Main Volume}, pages
  874--880, Online. Association for Computational Linguistics.

\bibitem[{Jiang and Bansal(2019)}]{jiang-bansal-2019-avoiding}
Yichen Jiang and Mohit Bansal. 2019.
\newblock \href {https://doi.org/10.18653/v1/P19-1262} {Avoiding reasoning
  shortcuts: Adversarial evaluation, training, and model development for
  multi-hop {QA}}.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 2726--2736, Florence, Italy.
  Association for Computational Linguistics.

\bibitem[{Johnson et~al.(2019)Johnson, Douze, and J{\'e}gou}]{faiss}
Jeff Johnson, Matthijs Douze, and Herv{\'e} J{\'e}gou. 2019.
\newblock Billion-scale similarity search with {GPUs}.
\newblock \emph{IEEE Transactions on Big Data}, 7(3):535--547.

\bibitem[{Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret}]{linear-transformers}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret.
  2020.
\newblock \href {https://doi.org/10.48550/ARXIV.2006.16236} {Transformers are
  rnns: Fast autoregressive transformers with linear attention}.

\bibitem[{Kedzie et~al.(2018)Kedzie, McKeown, and
  Daum{\'e}~III}]{kedzie-etal-2018-content}
Chris Kedzie, Kathleen McKeown, and Hal Daum{\'e}~III. 2018.
\newblock \href {https://doi.org/10.18653/v1/D18-1208} {Content selection in
  deep learning models of summarization}.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 1818--1828, Brussels, Belgium.
  Association for Computational Linguistics.

\bibitem[{Khandelwal et~al.(2019)Khandelwal, Levy, Jurafsky, Zettlemoyer, and
  Lewis}]{knnlm}
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis.
  2019.
\newblock \href {https://doi.org/10.48550/ARXIV.1911.00172} {Generalization
  through memorization: Nearest neighbor language models}.

\bibitem[{Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya}]{reformer}
Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. 2020.
\newblock \href {https://doi.org/10.48550/ARXIV.2001.04451} {Reformer: The
  efficient transformer}.

\bibitem[{Koh et~al.(2022)Koh, Ju, Liu, and Pan}]{empirical-longdoc-summary}
Huan~Yee Koh, Jiaxin Ju, Ming Liu, and Shirui Pan. 2022.
\newblock \href {https://doi.org/10.1145/3545176} {An empirical survey on long
  document summarization: Datasets, models, and metrics}.
\newblock \emph{ACM Comput. Surv.}, 55(8).

\bibitem[{Kryściński et~al.(2021)Kryściński, Rajani, Agarwal, Xiong, and
  Radev}]{booksum}
Wojciech Kryściński, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and
  Dragomir Radev. 2021.
\newblock \href {https://doi.org/10.48550/ARXIV.2105.08209} {Booksum: A
  collection of datasets for long-form narrative summarization}.

\bibitem[{Lee-Thorp et~al.(2021)Lee-Thorp, Ainslie, Eckstein, and
  Ontanon}]{fnet}
James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. 2021.
\newblock \href {https://doi.org/10.48550/ARXIV.2105.03824} {Fnet: Mixing
  tokens with fourier transforms}.

\bibitem[{Lewis et~al.(2020{\natexlab{a}})Lewis, Liu, Goyal, Ghazvininejad,
  Mohamed, Levy, Stoyanov, and Zettlemoyer}]{lewis2020bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020{\natexlab{a}}.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 7871--7880.

\bibitem[{Lewis et~al.(2020{\natexlab{b}})Lewis, Liu, Goyal, Ghazvininejad,
  Mohamed, Levy, Stoyanov, and Zettlemoyer}]{lewis-etal-2020-bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020{\natexlab{b}}.
\newblock \href {https://doi.org/10.18653/v1/2020.acl-main.703} {{BART}:
  Denoising sequence-to-sequence pre-training for natural language generation,
  translation, and comprehension}.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pages 7871--7880, Online. Association for
  Computational Linguistics.

\bibitem[{Lin(2004)}]{lin-2004-rouge}
Chin-Yew Lin. 2004.
\newblock \href {https://aclanthology.org/W04-1013} {{ROUGE}: A package for
  automatic evaluation of summaries}.
\newblock In \emph{Text Summarization Branches Out}, pages 74--81, Barcelona,
  Spain. Association for Computational Linguistics.

\bibitem[{Liu* et~al.(2018)Liu*, Saleh*, Pot, Goodrich, Sepassi, Kaiser, and
  Shazeer}]{wikisum}
Peter~J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz
  Kaiser, and Noam Shazeer. 2018.
\newblock \href {https://openreview.net/forum?id=Hyg0vbWC-} {Generating
  wikipedia by summarizing long sequences}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Liu and Lapata(2019)}]{liu-lapata-2019-hierarchical}
Yang Liu and Mirella Lapata. 2019.
\newblock \href {https://doi.org/10.18653/v1/P19-1500} {Hierarchical
  transformers for multi-document summarization}.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 5070--5081, Florence, Italy.
  Association for Computational Linguistics.

\bibitem[{Nallapati et~al.(2016)Nallapati, Zhou, dos Santos, Gul{\c{c}}ehre,
  and Xiang}]{nallapati-etal-2016-abstractive}
Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, {\c{C}}a{\u{g}}lar
  Gul{\c{c}}ehre, and Bing Xiang. 2016.
\newblock \href {https://doi.org/10.18653/v1/K16-1028} {Abstractive text
  summarization using sequence-to-sequence {RNN}s and beyond}.
\newblock In \emph{Proceedings of the 20th {SIGNLL} Conference on Computational
  Natural Language Learning}, pages 280--290, Berlin, Germany. Association for
  Computational Linguistics.

\bibitem[{Narayan et~al.(2018)Narayan, Cohen, and
  Lapata}]{narayan-etal-2018-dont}
Shashi Narayan, Shay~B. Cohen, and Mirella Lapata. 2018.
\newblock \href {https://doi.org/10.18653/v1/D18-1206} {Don{'}t give me the
  details, just the summary! topic-aware convolutional neural networks for
  extreme summarization}.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 1797--1807, Brussels, Belgium.
  Association for Computational Linguistics.

\bibitem[{Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu}]{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu. 2020.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21(1):5485--5551.

\bibitem[{Roy et~al.(2020)Roy, Saffar, Vaswani, and
  Grangier}]{routing-transformers}
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2020.
\newblock \href {https://doi.org/10.48550/ARXIV.2003.05997} {Efficient
  content-based sparse attention with routing transformers}.

\bibitem[{Shaham et~al.(2022)Shaham, Segal, Ivgi, Efrat, Yoran, Haviv, Gupta,
  Xiong, Geva, Berant, and Levy}]{scrolls}
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit
  Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. 2022.
\newblock \href {https://doi.org/10.48550/ARXIV.2201.03533} {Scrolls:
  Standardized comparison over long language sequences}.

\bibitem[{Tay et~al.(2020)Tay, Dehghani, Bahri, and
  Metzler}]{efficiency-survey}
Yi~Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020.
\newblock \href {https://doi.org/10.48550/ARXIV.2009.06732} {Efficient
  transformers: A survey}.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{vaswani-attn}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin. 2017.
\newblock \href
  {https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}
  {Attention is all you need}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30. Curran Associates, Inc.

\bibitem[{Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma}]{linformer}
Sinong Wang, Belinda~Z. Li, Madian Khabsa, Han Fang, and Hao Ma. 2020.
\newblock \href {https://doi.org/10.48550/ARXIV.2006.04768} {Linformer:
  Self-attention with linear complexity}.

\bibitem[{Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Le~Scao, Gugger, Drame, Lhoest, and Rush}]{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven Le~Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander Rush. 2020.
\newblock \href {https://doi.org/10.18653/v1/2020.emnlp-demos.6} {Transformers:
  State-of-the-art natural language processing}.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 38--45, Online.
  Association for Computational Linguistics.

\bibitem[{Wu et~al.(2022)Wu, Rabe, Hutchins, and Szegedy}]{memtrans}
Yuhuai Wu, Markus~Norman Rabe, DeLesley Hutchins, and Christian Szegedy. 2022.
\newblock \href {https://openreview.net/forum?id=TrjbxzRcnf-} {Memorizing
  transformers}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Xiao et~al.(2022)Xiao, Beltagy, Carenini, and
  Cohan}]{xiao-etal-2022-primera}
Wen Xiao, Iz~Beltagy, Giuseppe Carenini, and Arman Cohan. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-long.360} {{PRIMERA}:
  Pyramid-based masked sentence pre-training for multi-document summarization}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 5245--5263,
  Dublin, Ireland. Association for Computational Linguistics.

\bibitem[{Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, and Ahmed}]{big-bird}
Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti,
  Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li~Yang, and Amr
  Ahmed. 2020.
\newblock \href {https://doi.org/10.48550/ARXIV.2007.14062} {Big bird:
  Transformers for longer sequences}.

\bibitem[{Zhang et~al.(2021)Zhang, Negrinho, Ghosh, Jagannathan, Hassanzadeh,
  Schaaf, and Gormley}]{zhang-etal-2021-leveraging-pretrained}
Longxiang Zhang, Renato Negrinho, Arindam Ghosh, Vasudevan Jagannathan,
  Hamid~Reza Hassanzadeh, Thomas Schaaf, and Matthew~R. Gormley. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.findings-emnlp.313}
  {Leveraging pretrained models for automatic summarization of doctor-patient
  conversations}.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  EMNLP 2021}, pages 3693--3712, Punta Cana, Dominican Republic. Association
  for Computational Linguistics.

\bibitem[{Zhang et~al.(2019)Zhang, Kishore, Wu, Weinberger, and
  Artzi}]{bertscore}
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q. Weinberger, and Yoav Artzi.
  2019.
\newblock \href {https://doi.org/10.48550/ARXIV.1904.09675} {Bertscore:
  Evaluating text generation with bert}.

\bibitem[{Zhang et~al.(2022)Zhang, Ni, Mao, Wu, Zhu, Deb, Awadallah, Radev, and
  Zhang}]{zhang-etal-2022-summn}
Yusen Zhang, Ansong Ni, Ziming Mao, Chen~Henry Wu, Chenguang Zhu, Budhaditya
  Deb, Ahmed Awadallah, Dragomir Radev, and Rui Zhang. 2022.
\newblock \href {https://doi.org/10.18653/v1/2022.acl-long.112} {{S}umm$^n$: A
  multi-stage summarization framework for long input dialogues and documents}.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pages 1592--1604,
  Dublin, Ireland. Association for Computational Linguistics.

\bibitem[{Zhong et~al.(2022)Zhong, Lei, and Chen}]{zhong2022training}
Zexuan Zhong, Tao Lei, and Danqi Chen. 2022.
\newblock Training language models with memory augmentation.
\newblock In \emph{Empirical Methods in Natural Language Processing (EMNLP)}.

\end{thebibliography}
